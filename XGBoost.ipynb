{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clean_final_XGBoost.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9MVeca663mV","executionInfo":{"status":"ok","timestamp":1649504100096,"user_tz":-120,"elapsed":41241,"user":{"displayName":"Andre Nogueira Sousa","userId":"03467598651071349171"}},"outputId":"42882c6b-2273-479f-ac04-be6979634bf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package perluniprops to /root/nltk_data...\n","[nltk_data]   Unzipping misc/perluniprops.zip.\n","[nltk_data] Downloading package nonbreaking_prefixes to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Collecting es_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n","\u001b[K     |████████████████████████████████| 16.2 MB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.63.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n","Building wheels for collected packages: es-core-news-sm\n","  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-py3-none-any.whl size=16172933 sha256=a8fd33b0f527270e1f1f3494fadef64d91a3d2a961bc98b99f7b092df6ed3fee\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m6jrezfv/wheels/21/8d/a9/6c1a2809c55dd22cd9644ae503a52ba6206b04aa57ba83a3d8\n","Successfully built es-core-news-sm\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('es_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/es_core_news_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/es\n","You can now load the model via spacy.load('es')\n","Mounted at /content/drive/\n"]}],"source":["import spacy\n","import nltk\n","import string\n","import re\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","\n","from collections import Counter\n","from xgboost import XGBClassifier\n","from nltk.tokenize.casual import TweetTokenizer\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import SnowballStemmer\n","from sklearn import metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"]},{"cell_type":"code","source":["nltk.download('perluniprops')\n","nltk.download('nonbreaking_prefixes')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","!python -m spacy download es"],"metadata":{"id":"RJiRgqfKloVU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reading the pickle with the data already transformed"],"metadata":{"id":"M2mSrQLF9I4g"}},{"cell_type":"code","source":["df_train = pd.read_pickle('./data/df_train.pickle')\n","df_train = df_train[df_train['final_label']!='unknown']\n","df_test = pd.read_pickle('./data/df_test.pickle')"],"metadata":{"id":"lbIT_Z5bm2YT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function that executes the tf-idf and runs the XGBoost"],"metadata":{"id":"kzzSF4Td9OFw"}},{"cell_type":"code","source":["def xgb_tune(data,data_test,fraction,column,n_features,learning_rate,n_estimators,max_depth,min_child_weight,subsample,colsample_bytree,max_df,min_df,gamma,la):\n","  if fraction > 0.99:\n","    training_data=data\n","    testing_data=data_test\n","  else:\n","    training_data = data.sample(frac=fraction, random_state=25)\n","    testing_data = pd.concat([data.drop(training_data.index),data_test])\n","  tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df =min_df, max_features=n_features, use_idf=True)\n","  tfidf_vectorizer = tfidf_vectorizer.fit(training_data[column])\n","  tfidf_train = tfidf_vectorizer.transform(training_data[column])\n","  tfidf_test = tfidf_vectorizer.transform(testing_data[column])\n","  X_train = tfidf_train.toarray()\n","  y_train = training_data['final_label'].map({'non-racist': 1, 'racist' :0}).to_numpy()\n","  X_test = tfidf_test.toarray()\n","  y_test = testing_data['final_label'].map({'non-racist': 1, 'racist' :0}).to_numpy()\n","  model = XGBClassifier(random_state=42, learning_rate=learning_rate,n_estimators=n_estimators,max_depth=max_depth,min_child_weight=min_child_weight,subsample=subsample,colsample_bytree=colsample_bytree,gamma=gamma,reg_lambda=la)\n","  results = model.fit(X_train,y_train)\n","  preds = results.predict(X_test)\n","  return results,preds,y_test,tfidf_vectorizer"],"metadata":{"id":"fzZ8-0mBXlHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training the XGBoost with the df_test data"],"metadata":{"id":"1xB8OFbeGb5N"}},{"cell_type":"code","source":["results,preds,y_test,tfidf_vectorizer = xgb_tune(df_train,df_test,1.0,'stemm_str',5000,0.2,100,10,1,0.8,0.8,0.9,2,0,1)\n","X_test = tfidf_vectorizer.transform(df_test['stemm_str']).toarray()\n","y_test = df_test['final_label'].map({'non-racist': 1, 'racist' :0}).to_numpy()\n","preds_test = results.predict(X_test)\n","print(f'f1-score of the test: {metrics.f1_score(y_test, preds_test)}')\n","X_train = tfidf_vectorizer.transform(df_train['stemm_str']).toarray()\n","y_train = df_train['final_label'].map({'non-racist': 1, 'racist' :0}).to_numpy()\n","preds_sample = results.predict(X_train)\n","print(f'f1-score of the train: {metrics.f1_score(y_train, preds_sample)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiDlCVGX2dhJ","executionInfo":{"status":"ok","timestamp":1649504343086,"user_tz":-120,"elapsed":168402,"user":{"displayName":"Andre Nogueira Sousa","userId":"03467598651071349171"}},"outputId":"8283e058-b6c0-449f-c9e0-b273c9f8dd1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f1-score of the test: 0.8999999999999999\n","f1-score of the train: 0.9355563881603597\n"]}]},{"cell_type":"markdown","source":["Reading the data from the evaluation_final.csv file and preparing it for the XGBoost"],"metadata":{"id":"bLI4OfbEF-Qi"}},{"cell_type":"code","source":["df_evaluation = pd.read_csv('./data/evaluation_final_original.csv', sep='|', header=0)\n","df_evaluation = df_evaluation.loc[:, ['message', 'label']]\n","\n","def normalize(text,nlp):\n","  doc = nlp(text)\n","  words = [t.lemma_ for t in doc if not t.is_punct | t.is_stop]\n","  lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n","  return lexical_tokens\n","\n","def get_wordnet_pos(tag):\n","  if tag.startswith('J'):\n","      return wordnet.ADJ\n","  elif tag.startswith('V'):\n","      return wordnet.VERB\n","  elif tag.startswith('N'):\n","      return wordnet.NOUN\n","  elif tag.startswith('R'):\n","      return wordnet.ADV\n","  else:\n","      return wordnet.NOUN\n","\n","t = TweetTokenizer()\n","df_evaluation['tokenized'] = df_evaluation['message'].apply(t.tokenize)\n","df_evaluation['lower'] = df_evaluation['tokenized'].apply(lambda x: [word.lower() for word in x])\n","punc = string.punctuation+'...¿¡..“'\n","df_evaluation['no_punc'] = df_evaluation['lower'].apply(lambda x: [word for word in x if word not in punc])\n","stop_words = set(stopwords.words('spanish'))\n","df_evaluation['stopwords_removed'] = df_evaluation['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n","df_evaluation['pos_tags'] = df_evaluation['stopwords_removed'].apply(nltk.tag.pos_tag)\n","df_evaluation['wordnet_pos'] = df_evaluation['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n","wnl = WordNetLemmatizer()\n","df_evaluation['lemmatized'] = df_evaluation['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n","df_evaluation['lemma_str'] = [' '.join(map(str,l)) for l in df_evaluation['lemmatized']]\n","spanish_stemmer = SnowballStemmer('spanish')\n","df_evaluation['stemm'] = df_evaluation['wordnet_pos'].apply(lambda x: [spanish_stemmer.stem(word) for word, tag in x])\n","df_evaluation['stemm_str'] = [' '.join(map(str,l)) for l in df_evaluation['stemm']]\n","nlp = spacy.load('es')\n","df_evaluation['lemma_spacy'] = df_evaluation['message'].apply(lambda x: normalize(x,nlp))\n","df_evaluation['lemma_spacy_str'] = [' '.join(map(str,l)) for l in df_evaluation['lemma_spacy']]\n","tweet_len = []\n","for index, row in df_evaluation.iterrows():\n","    tweet_len.append(len(row['lemma_str']))\n","df_evaluation['tweet_len'] = tweet_len\n","df_evaluation['word_count'] = df_evaluation['lemmatized'].apply(lambda x: len(str(x).split()))"],"metadata":{"id":"EdEzlKb7Wtvp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generating the tf-idf for the evaluation final usgin the vector"],"metadata":{"id":"BPOfshizGGV_"}},{"cell_type":"code","source":["tfidf_evalutation = tfidf_vectorizer.transform(df_evaluation['stemm_str'])\n","X_eval = tfidf_evalutation.toarray()\n","preds = results.predict(X_eval)"],"metadata":{"id":"BrvAOB9yWihX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Saving the csv file with the results"],"metadata":{"id":"rTZNw6hkGObM"}},{"cell_type":"code","source":["df_evaluation_print = pd.read_csv('./data/evaluation_final_original.csv', sep='|', header=0)\n","df_evaluation_print['label'] = preds\n","df_evaluation_print['label'] = df_evaluation_print['label'].map({1 : 'non-racist', 0: 'racist'})\n","df_evaluation_print.to_csv('./data/evaluation_final_xgb1.csv')"],"metadata":{"id":"vj1gTn_8jOZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Saving model and vector to be used in ensemble"],"metadata":{"id":"PtSWmZ1F9xHU"}},{"cell_type":"code","source":["import pickle\n","pickle.dump(results, open('./models/xgb_reg_best.pkl', \"wb\"))\n","pickle.dump(tfidf_vectorizer, open('./models/xgb_tfidf_vec_best.pkl', \"wb\"))"],"metadata":{"id":"D3KlQpaytSdY"},"execution_count":null,"outputs":[]}]}